#!/usr/bin/env python3
"""
FinanceQA LLM Evaluation Script

Standalone script for performing LLM-based evaluation on existing agent result files.
This script takes result files generated by run_agent_evaluation.py and adds LLM scoring.
"""

import sys
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional

import pandas as pd
from tqdm import tqdm

# Import shared utilities
from .shared_evaluation_utils import (
    EvaluationExample, AgentResponse, EvaluationResult,
    load_results, save_results, validate_result_file,
    create_base_argument_parser, add_llm_evaluation_arguments,
    setup_model_manager, setup_llm_scorer, compute_metrics_llm_only,
    validate_evaluation_arguments
)


class LLMEvaluator:
    """LLM-based evaluator for agent result files."""

    def __init__(self, model_manager, evaluation_model: Optional[str] = None, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the LLM evaluator.

        Args:
            model_manager: Model manager for LLM evaluation
            evaluation_model: Specific model to use for evaluation (optional)
            config: Configuration dictionary with evaluation settings (optional)
        """
        self.scorer = setup_llm_scorer(model_manager, evaluation_model, config)

    def _extract_final_answer(self, agent_response_text: str) -> str:
        """
        Extract the final answer from agent response text.

        Args:
            agent_response_text: Full agent response including thinking and formatting

        Returns:
            Extracted final answer or original text if no clear final answer found
        """
        if not agent_response_text or agent_response_text.strip() == "<tool_call>":
            return ""

        # Remove <think> blocks first
        text = re.sub(r'<think>.*?</think>', '', agent_response_text, flags=re.DOTALL)

        # Common patterns for final answers
        patterns = [
            # <final_answer>: X
            r'<final_answer>[:\s]*(.+?)(?:\n|$)',
            # <answer>: X
            r'<answer>[:\s]*(.+?)(?:\n|$)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                final_answer = match.group(1).strip()
                # Clean up common formatting
                final_answer = re.sub(r'\*+', '', final_answer)  # Remove asterisks
                final_answer = final_answer.strip('.,;')  # Remove trailing punctuation
                if final_answer:
                    return final_answer

        # If no explicit final answer pattern, look for currency/number patterns at the end
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if lines:
            # Look for lines with financial data (numbers, currency, percentages)
            for line in reversed(lines):
                if re.search(r'[\d$%]', line) and len(line) < 200:
                    # Clean up the line
                    clean_line = re.sub(r'^[*\-•]+\s*', '', line)  # Remove bullet points
                    clean_line = clean_line.strip('.,;')
                    if clean_line:
                        return clean_line

        # Fallback: return original text truncated if reasonable length
        clean_text = text.strip()
        if len(clean_text) < 300:
            return clean_text

        # Last resort: return first meaningful paragraph
        paragraphs = [p.strip() for p in clean_text.split('\n\n') if p.strip()]
        if paragraphs:
            return paragraphs[0][:200] + "..." if len(paragraphs[0]) > 200 else paragraphs[0]

        return clean_text[:200] + "..." if len(clean_text) > 200 else clean_text

    def evaluate_result_file(
        self,
        result_file_path: str,
        failed_examples: str = "process",
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        Evaluate agent results using LLM scoring.

        Args:
            result_file_path: Path to the result file to evaluate
            failed_examples: How to handle failed agent examples ("skip", "process", "flag")
            verbose: Whether to show progress bars

        Returns:
            Dictionary containing updated evaluation results
        """
        print(f"Loading result file: {result_file_path}")

        # Load and validate result file
        try:
            result_data = load_results(result_file_path)
            if not validate_result_file(result_data):
                raise ValueError("Invalid result file format")
        except Exception as e:
            raise Exception(f"Failed to load result file: {e}")

        print(f"Loaded {len(result_data['detailed_results'])} results")

        # Convert results back to EvaluationResult objects
        results = self._convert_to_evaluation_results(result_data['detailed_results'])

        # Filter based on failed_examples setting
        if failed_examples == "skip":
            original_count = len(results)
            results = [r for r in results if r.error_message is None]
            print(f"Skipping {original_count - len(results)} failed examples")
        elif failed_examples == "process":
            print("Processing all examples including failed ones")
        elif failed_examples == "flag":
            print("Flagging failed examples (processing all)")

        if not results:
            raise ValueError("No results to evaluate after filtering")

        # Run LLM evaluation
        print("Running LLM evaluation...")
        evaluated_results = self._run_llm_evaluation(results, failed_examples, verbose)

        # Compute LLM-based metrics
        metrics = compute_metrics_llm_only(evaluated_results)

        # Prepare output
        evaluation_output = {
            'results': evaluated_results,
            'metrics': metrics,
            'total_examples': len(evaluated_results),
            'total_processing_time': result_data.get('total_processing_time', 0.0),
            'average_processing_time': result_data.get('average_processing_time', 0.0)
        }

        return evaluation_output

    def _convert_to_evaluation_results(self, detailed_results: List[Dict[str, Any]]) -> List[EvaluationResult]:
        """
        Convert serialized results back to EvaluationResult objects.

        Args:
            detailed_results: List of serialized result dictionaries

        Returns:
            List of EvaluationResult objects
        """
        results = []

        for result_dict in detailed_results:
            # Reconstruct EvaluationExample
            example = EvaluationExample(
                context="",  # Not stored in detailed results
                question=result_dict['question'],
                chain_of_thought="",  # Not stored in detailed results
                answer=result_dict['ground_truth'],
                file_link="",  # Not stored in detailed results
                file_name="",  # Not stored in detailed results
                company=result_dict.get('company', ''),
                question_type=result_dict.get('question_type', '')
            )

            # Reconstruct AgentResponse
            agent_response = AgentResponse(
                answer=result_dict['predicted_answer'],
                reasoning_steps=result_dict.get('reasoning_steps', []),
                confidence_score=result_dict.get('confidence_score', 0.5),
                processing_time=result_dict.get('processing_time', 0.0)
            )

            # Create EvaluationResult
            result = EvaluationResult(
                example=example,
                agent_response=agent_response,
                processing_time=result_dict.get('processing_time', 0.0),
                error_message=result_dict.get('error_message'),
                llm_match=result_dict.get('llm_match'),
                llm_reasoning=result_dict.get('llm_reasoning')
            )

            results.append(result)

        return results

    def _run_llm_evaluation(
        self,
        results: List[EvaluationResult],
        failed_examples: str,
        verbose: bool
    ) -> List[EvaluationResult]:
        """
        Run LLM evaluation on the results.

        Args:
            results: List of evaluation results
            failed_examples: How to handle failed examples
            verbose: Whether to show progress bars

        Returns:
            List of evaluation results with LLM scores
        """
        evaluated_results = []

        # Progress bar
        iterator = tqdm(results, desc="LLM Evaluation") if verbose else results

        for result in iterator:
            try:
                # Skip LLM evaluation for failed examples if requested
                if failed_examples == "skip" and result.error_message is not None:
                    result.llm_match = False
                    result.llm_reasoning = f"Skipped due to agent error: {result.error_message}"
                elif result.error_message is not None and failed_examples == "flag":
                    result.llm_match = False
                    result.llm_reasoning = f"Agent failed: {result.error_message}"
                else:
                    # Extract clean final answer from agent response
                    clean_predicted_answer = self._extract_final_answer(result.agent_response.answer)

                    # Run LLM evaluation with cleaned answer
                    llm_result = self.scorer.evaluate_answer_pair(
                        predicted=clean_predicted_answer,
                        ground_truth=result.example.answer,
                        question=result.example.question
                    )

                    result.llm_match = llm_result.is_correct
                    result.llm_reasoning = llm_result.reasoning

                    # Add confidence and error info if available
                    if hasattr(llm_result, 'confidence'):
                        result.llm_reasoning += f" (Confidence: {llm_result.confidence:.2f})"

                    if llm_result.error_message:
                        result.llm_reasoning += f" [LLM Error: {llm_result.error_message}]"

            except Exception as e:
                # Handle LLM evaluation errors
                result.llm_match = False
                result.llm_reasoning = f"LLM evaluation failed: {str(e)}"

            evaluated_results.append(result)

        return evaluated_results

    def save_evaluated_results(self, evaluation_output: Dict[str, Any], output_path: str):
        """
        Save LLM-evaluated results to a file.

        Args:
            evaluation_output: Output from evaluate_result_file()
            output_path: Path to save evaluated results
        """
        save_results(evaluation_output, output_path)

    def print_evaluation_summary(self, evaluation_output: Dict[str, Any], input_file: str):
        """
        Print LLM evaluation summary.

        Args:
            evaluation_output: Output from evaluate_result_file()
            input_file: Original input file path
        """
        metrics = evaluation_output['metrics']

        print(f"\nLLM EVALUATION SUMMARY")
        print("=" * 60)
        print(f"Input File: {Path(input_file).name}")
        print(f"Total Examples: {metrics['total_examples']}")
        print(f"LLM Match Accuracy: {metrics['llm_match_accuracy']:.1%}")
        print(f"LLM Matches: {metrics['llm_matches']}")
        print(f"Error Rate: {metrics['error_rate']:.1%}")

        # Per question type breakdown
        if 'by_question_type' in metrics and metrics['by_question_type']:
            print("\nLLM Accuracy by Question Type:")
            print("-" * 50)
            for qtype, type_metrics in metrics['by_question_type'].items():
                print(f"{qtype:20s}: {type_metrics['llm_match_accuracy']:.1%} "
                      f"({type_metrics['total_examples']} examples)")


def main():
    """Command-line interface for LLM evaluation."""
    # Create argument parser
    parser = create_base_argument_parser("FinanceQA LLM Evaluation")
    parser = add_llm_evaluation_arguments(parser)

    args = parser.parse_args()

    try:
        # Setup model manager for LLM evaluation
        print("🔧 Setting up model manager for LLM evaluation...")
        model_manager, config = setup_model_manager()
        print(f"✅ Model manager setup complete using {config['default_provider']}")

        # Setup LLM evaluator
        print("🧠 Setting up LLM evaluator...")

        # Use evaluation-specific model if configured
        eval_model = args.evaluation_model
        if not eval_model and 'evaluation' in config:
            eval_config = config['evaluation']
            eval_model = eval_config.get('model_name')
            print(f"📊 Using evaluation config: {eval_config.get('provider')} / {eval_model}")

        evaluator = LLMEvaluator(model_manager, eval_model, config)
        print("✅ LLM evaluator setup complete")

    except Exception as e:
        print(f"❌ Failed to setup LLM evaluator: {e}")
        sys.exit(1)

    verbose = not args.no_verbose

    try:
        # Run LLM evaluation
        evaluation_output = evaluator.evaluate_result_file(
            result_file_path=args.result_file,
            failed_examples=args.failed_examples,
            verbose=verbose
        )

        # Generate output filename
        input_path = Path(args.result_file)
        if args.output_file:
            output_file = args.output_file
        else:
            # Add _llm_evaluated suffix to original filename
            stem = input_path.stem
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"{stem}_llm_evaluated_{timestamp}.json"

        # Save results
        output_path = Path(args.output_dir) / output_file
        output_path.parent.mkdir(exist_ok=True)
        evaluator.save_evaluated_results(evaluation_output, str(output_path))

        # Print summary
        evaluator.print_evaluation_summary(evaluation_output, args.result_file)

        print(f"\n✅ LLM evaluation completed successfully!")
        print(f"📄 Evaluated results saved to: {output_path}")

    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("Make sure the result file exists and is in the correct location.")
        sys.exit(1)
    except Exception as e:
        print(f"LLM evaluation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()